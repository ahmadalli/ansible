---
- name: gather os specific variables for package
  ansible.builtin.include_vars: "{{ item }}"
  with_first_found:
    - package/{{ ansible_distribution|lower }}.yaml
    - package/{{ ansible_os_family|lower }}.yaml
    - package/default.yaml
  tags:
    - facts
- name: install opennebula node packages
  ansible.builtin.include_role:
    name: utils/os/package/manager
- name: remove virsh default network
  ansible.builtin.shell: virsh net-destroy default; virsh net-undefine default
  ignore_errors: true
- name: include network vars
  ansible.builtin.include_vars: network.yaml
- name: setup network
  ansible.builtin.include_role:
    name: utils/os/network/configure
# we do this because br_netfilter might get loaded in future and if it does, it'll ruin our connectivity without proper configs
- name: persist br_netfilter module
  ansible.builtin.lineinfile:
    path: /etc/modules
    regexp: ^br_netfilter$
    line: br_netfilter
- name: load br_netfilter module
  community.general.modprobe:
    name: br_netfilter
    state: present
# https://wiki.libvirt.org/page/Networking#Creating_network_initscripts
- name: disable netfilter on bridges
  ansible.posix.sysctl:
    sysctl_file: /etc/sysctl.d/zzz-opennebula-bridge-nf-call.conf
    name: "{{ item }}"
    reload: yes
    value: "0"
    sysctl_set: yes
  loop:
    - net.bridge.bridge-nf-call-ip6tables
    - net.bridge.bridge-nf-call-iptables
    - net.bridge.bridge-nf-call-arptables
- name: setup glusterfs volume mount
  ansible.builtin.include_role:
    name: glusterfs/mount
- name: check state of datastore directory
  ansible.builtin.stat:
    path: /var/lib/one/datastores
  register: datastore_dir_stat
- name: remove current datastore dir if not link or pointing at wrong location
  file:
    state: absent
    path: /var/lib/one/datastores
  when: |
    not (datastore_dir_stat.stat.islnk | default(false, true))
    or datastore_dir_stat.stat.lnk_source != "{{ opennebula_node_datastore_glusterfs_mount_path }}/datastores"
- name: create datastore softlink
  ansible.builtin.file:
    src: "{{ opennebula_node_datastore_glusterfs_mount_path }}/datastores"
    dest: /var/lib/one/datastores
    owner: oneadmin
    group: oneadmin
    follow: no
    state: link
- name: disable and stop lvmetad
  ansible.builtin.systemd:
    name: lvm2-lvmetad.service
    enabled: false
    state: stopped
  ignore_errors: true
- name: make sure lvm disks are set
  when: opennebula_datastore_lvm_devices | default([], true) | length == 0
  ansible.builtin.fail:
    msg: |
      opennebula_datastore_lvm_devices must be set
- name: check lvm disks
  include_tasks: disk-check.yaml
  loop: "{{ opennebula_datastore_lvm_devices }}"
  loop_control:
    loop_var: _disk
- name: create lvm group
  community.general.lvg:
    vg: vg-one-{{ opennebula_datastore_system_id }}
    pvs: "{{ opennebula_datastore_lvm_devices }}"
- name: add frontend public key
  authorized_key:
    user: oneadmin
    key: "{{ opennebula_frontend_authorized_key }}"
    exclusive: yes
    state: present
    manage_dir: no
  when: |
    opennebula_add_frontend_authorized_key and
    (opennebula_frontend_authorized_key | default('', true) | length > 0)
- name: list current nodes
  tags:
    - add-node
  delegate_to: "{{ groups['opennebula_frontend'][0] }}"
  shell: onehost list --csv | cut -d',' -f2 | sed 1d
  register: __current_nodes
  failed_when: __current_nodes.rc != 0
  changed_when: false
- name: propagate node hosts to /etc/hosts on frontend
  delegate_to: "{{ groups['opennebula_frontend'][0] }}"
  ansible.builtin.blockinfile:
    path: /etc/hosts
    marker: "# {mark} ANSIBLE MANAGED BLOCK nebula nodes"
    block: |
      {% for host in groups['opennebula_node'] %}
      {{ hostvars[host]['ip'] | default(hostvars[host]['ansible_host'] , true) }} {{ host }}
      {% endfor %}
- name: add node to cluster
  tags:
    - add-node
  delegate_to: "{{ groups['opennebula_frontend'][0] }}"
  command: onehost create {{ inventory_hostname }} --im kvm --vm kvm
  when:
    - __current_nodes.stdout_lines is defined
    - not inventory_hostname in __current_nodes.stdout_lines
